{
  "rag_enabled": true,
  "chunk_size": 500,
  "chunk_overlap": 50,
  "top_k": 3,
  "min_retrieval_score": 0.3,
  
  "index_path": "./data/rag_index",
  "docs_path": "./data/docs",
  
  "rag_disable_keyword": "disable-rag",
  
  "embedding_model_path": "/Users/kayisrahman/llama.cpp/hf-models/nomic-embed-text-v1.5.Q8_0.gguf",
  "embedding_model_name": "embedding",
  "embedding_n_ctx": 2048,
  "embedding_n_gpu_layers": -1,
  "embedding_cache_enabled": true,
  "embedding_cache_size": 1000,
  
  "external_chat_api_url": "https://u425-afb3-687d7019.singapore-a.gpuhub.com:8443/v1/chat/completions",
  "external_chat_api_key": "",
  "use_external_chat_model": false,
  
  "temperature": 0.7,
  "max_tokens": 2048,
  
  "rag_api_port": 8001,
  "rag_api_host": "0.0.0.0",
  
  "_comment": "Set embedding_model_path to your GGUF embedding model file. Set use_external_chat_model to true and configure external_chat_api_url to use external LLM server."
}
