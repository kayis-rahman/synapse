["# Rag Local Provider (Path A) for opencode CLI ## Overview This document describes a local Rag provider integration that exposes Rag’s API via a Python FastAPI server and wires it into the opencode CLI in a local, llama.cpp/Ollama-like UX. The server runs on macOS, uses on-device embeddings and a CPU FAISS-like vector store, and delegates LLM generation to OpenRouter via a cloud API key stored in environment variables. ## Architecture Summary - Local Rag server: Python FastAPI with", "OpenAI-compatible endpoints: - GET /health: readiness check - GET /v1/models: list available models - POST /v1/chat/completions: processes a query and returns OpenAI-style chat completion - Response: { \"id\": \"...\", \"object\": \"chat.completion\", \"created\": int, \"model\": \"...\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"answer\"}, \"finish_reason\": \"stop\"}] } - POST /v1/completions: text completion - Rag core: EmbeddingService, VectorStore, Retriever, LLMController,", "RagOrchestrator - Rag provider for opencode: Configured as OpenAI-compatible provider - Base URL: http://localhost:8000 - Model: mistralai/devstral-2512:free - API Key: OpenRouter key for LLM generation ## Local Server Contract (Path A) - Base URL: http://localhost:8000 - Endpoints: - GET /health -> {\"status\": \"ok\"} - GET /v1/models -> {\"object\": \"list\", \"data\": [{\"id\": \"mistralai/devstral-2512:free\", ...}]} - POST /v1/chat/completions - Body: {\"model\": \"mistralai/devstral-2512:free\",", "\"messages\": [{\"role\": \"user\", \"content\": \"query\"}]} - Response: OpenAI chat completion format with Rag-generated answer - Auth: API key sent in request (not checked by server) ## Env & Configuration - Rag server: - RAG_INDEX_PATH: path to index/docs on disk (default: ./rag_index) - RAG_TOP_K: number of top docs to retrieve (default: 5) - OPENROUTER_API_KEY: API key for LLM generation via OpenRouter - opencode config (~/.config/opencode/opencode.jsonc): - Provider: pi-rag - baseURL:", "\"http://localhost:8000\" - apiKey: OpenRouter key - Models: {\"mistralai/devstral-2512:free\": {\"name\": \"Mistral Devstral 2512 Free\"}} ## opencode Integration (Provider Surface) - Configured as OpenAI-compatible provider in opencode config - Provider name: pi-rag - Uses @ai-sdk/openai-compatible npm package - Command: opencode run -m pi-rag/mistralai/devstral-2512:free \"query\" ## How to Run (Mac) - Start server: - python3 -m uvicorn rag_server.main:app --reload --host 127.0.0.1 --port 8000 - Health", "check: - curl http://127.0.0.1:8000/health - Query via opencode: - opencode run -m pi-rag/mistralai/devstral-2512:free \"What is Rag?\" - Manual query: - curl -X POST http://127.0.0.1:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"model\": \"mistralai/devstral-2512:free\", \"messages\": [{\"role\": \"user\", \"content\": \"What is Rag?\"}]}' ## Security Considerations - Do not commit OPENROUTER_API_KEY. Use env vars. - Bind to localhost; consider additional token if you expose externally ##", "Extensibility - Swap embedding models or vector stores if needed - Add caching, batching, or a simple UI wrapper later ## Pi Porting Plan (Phase 2) - Containerize with Docker ARM64 for Raspberry Pi - Ensure Python 3 and dependencies available - Reuse opencode config pointing to Pi-hosted server ## Patch Summary - This document describes Path A local Rag server + opencode provider integration for a seamless, local-LM UX.", "# Plan: Rag Local Provider for opencode CLI (Path A) ## Goal Integrate Rag as a local provider for opencode CLI using a Python FastAPI server on macOS, with on-device embeddings and CPU FAISS-like vector store. Port to Raspberry Pi later. ## Milestones - M1: Local Rag server implemented with OpenAI-compatible endpoints at localhost:8000 ✅ - M2: opencode configured as OpenAI-compatible provider ✅ - M3: opencode integration working, queries Rag server and prints answers ✅ - M4: Pi-port plan", "defined (Docker ARM64) ✅ - M5: Documentation finalized (Agent.md, updated Plan.md) ✅ ## Tasks - T1: Ensure robust importer setup for rag modules ✅ - T2: Verify /health and OpenAI endpoints on macOS ✅ - T3: Configure opencode as OpenAI-compatible provider ✅ - T4: Prepare documentation for running the local Rag server ✅ - T5: Define Pi-port strategy (Docker ARM64) ✅ ## Risks & Mitigations - RAG OpenRouter latency: use a caching layer for frequently asked questions - Pi memory constraints: keep a", "small index and enable batching - Secrets management: env-var based; rotate keys per policy ## Acceptance Criteria - macOS: local Rag server accessible via OpenAI-compatible endpoints, returns answers ✅ - opencode: configured and invokes Rag server, prints answers ✅ - Pi: Docker ARM64 strategy defined ✅", "INFO: LLM request to OpenRouter succeeded for model mistralai/devstral-2512:free ERROR: API key unauthorized - check OpenRouter key DEBUG: Embedding loaded for sentence-transformers/all-MiniLM-L6-v2 WARNING: Vector store empty - no docs indexed yet INFO: Chat completion streamed successfully ERROR: Failed to parse JSON in streaming response", "# Rag Local Provider (Path A) for opencode CLI ## Overview This document describes a local Rag provider integration that exposes Rag’s API via a Python FastAPI server and wires it into the opencode CLI in a local, llama.cpp/Ollama-like UX. The server runs on macOS, uses on-device embeddings and a CPU FAISS-like vector store, and delegates LLM generation to OpenRouter via a cloud API key stored in environment variables. ## Architecture Summary - Local Rag server: Python FastAPI with", "OpenAI-compatible endpoints: - GET /health: readiness check - GET /v1/models: list available models - POST /v1/chat/completions: processes a query and returns OpenAI-style chat completion - Response: { \"id\": \"...\", \"object\": \"chat.completion\", \"created\": int, \"model\": \"...\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"answer\"}, \"finish_reason\": \"stop\"}] } - POST /v1/completions: text completion - Rag core: EmbeddingService, VectorStore, Retriever, LLMController,", "RagOrchestrator - Rag provider for opencode: Configured as OpenAI-compatible provider - Base URL: http://localhost:8000 - Model: mistralai/devstral-2512:free - API Key: OpenRouter key for LLM generation ## Local Server Contract (Path A) - Base URL: http://localhost:8000 - Endpoints: - GET /health -> {\"status\": \"ok\"} - GET /v1/models -> {\"object\": \"list\", \"data\": [{\"id\": \"mistralai/devstral-2512:free\", ...}]} - POST /v1/chat/completions - Body: {\"model\": \"mistralai/devstral-2512:free\",", "\"messages\": [{\"role\": \"user\", \"content\": \"query\"}]} - Response: OpenAI chat completion format with Rag-generated answer - Auth: API key sent in request (not checked by server) ## Env & Configuration - Rag server: - RAG_INDEX_PATH: path to index/docs on disk (default: ./rag_index) - RAG_TOP_K: number of top docs to retrieve (default: 5) - OPENROUTER_API_KEY: API key for LLM generation via OpenRouter - opencode config (~/.config/opencode/opencode.jsonc): - Provider: pi-rag - baseURL:", "\"http://localhost:8000\" - apiKey: OpenRouter key - Models: {\"mistralai/devstral-2512:free\": {\"name\": \"Mistral Devstral 2512 Free\"}} ## opencode Integration (Provider Surface) - Configured as OpenAI-compatible provider in opencode config - Provider name: pi-rag - Uses @ai-sdk/openai-compatible npm package - Command: opencode run -m pi-rag/mistralai/devstral-2512:free \"query\" ## How to Run (Mac) - Start server: - python3 -m uvicorn rag_server.main:app --reload --host 127.0.0.1 --port 8000 - Health", "check: - curl http://127.0.0.1:8000/health - Query via opencode: - opencode run -m pi-rag/mistralai/devstral-2512:free \"What is Rag?\" - Manual query: - curl -X POST http://127.0.0.1:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\"model\": \"mistralai/devstral-2512:free\", \"messages\": [{\"role\": \"user\", \"content\": \"What is Rag?\"}]}' ## Security Considerations - Do not commit OPENROUTER_API_KEY. Use env vars. - Bind to localhost; consider additional token if you expose externally ##", "Extensibility - Swap embedding models or vector stores if needed - Add caching, batching, or a simple UI wrapper later ## Pi Porting Plan (Phase 2) - Containerize with Docker ARM64 for Raspberry Pi - Ensure Python 3 and dependencies available - Reuse opencode config pointing to Pi-hosted server ## Patch Summary - This document describes Path A local Rag server + opencode provider integration for a seamless, local-LM UX.", "# Plan: Rag Local Provider for opencode CLI (Path A) ## Goal Integrate Rag as a local provider for opencode CLI using a Python FastAPI server on macOS, with on-device embeddings and CPU FAISS-like vector store. Port to Raspberry Pi later. ## Milestones - M1: Local Rag server implemented with OpenAI-compatible endpoints at localhost:8000 ✅ - M2: opencode configured as OpenAI-compatible provider ✅ - M3: opencode integration working, queries Rag server and prints answers ✅ - M4: Pi-port plan", "defined (Docker ARM64) ✅ - M5: Documentation finalized (Agent.md, updated Plan.md) ✅ ## Tasks - T1: Ensure robust importer setup for rag modules ✅ - T2: Verify /health and OpenAI endpoints on macOS ✅ - T3: Configure opencode as OpenAI-compatible provider ✅ - T4: Prepare documentation for running the local Rag server ✅ - T5: Define Pi-port strategy (Docker ARM64) ✅ ## Risks & Mitigations - RAG OpenRouter latency: use a caching layer for frequently asked questions - Pi memory constraints: keep a", "small index and enable batching - Secrets management: env-var based; rotate keys per policy ## Acceptance Criteria - macOS: local Rag server accessible via OpenAI-compatible endpoints, returns answers ✅ - opencode: configured and invokes Rag server, prints answers ✅ - Pi: Docker ARM64 strategy defined ✅", "# Rag local server package", "Rag Local Server (Python FastAPI) This local server exposes a Rag query API to be consumed by the opencode CLI as a local provider. It uses on-device embeddings and a FAISS-like vector store, and delegates LLM generation to Rag OpenRouter API via the OPENROUTER_API_KEY env var. Usage: - Ensure prerequisites: Python 3.x, uvicorn, fastapi - Install server requirements: - pip install fastapi uvicorn - Start server: - RAG_INDEX_PATH=./rag_index RAG_TOP_K=5 OPENROUTER_API_KEY=<your-key> uvicorn", "rag_server.main:app --reload --port 8000 - Health check: - curl http://localhost:8000/health - Query: - curl -sS -X POST http://localhost:8000/query -H 'Content-Type: application/json' -d '{\"query\": \"What is Rag?\", \"top_k\": 5}' | jq Note: Do not commit OpenRouter API keys. Use environment variable OPENROUTER_API_KEY.", "from fastapi import FastAPI, Request from fastapi.responses import StreamingResponse from pydantic import BaseModel from typing import List, Optional import os import logging logging.basicConfig(filename='rag_server.log', level=logging.DEBUG) # Import Rag components from rag.embedding import EmbeddingService from rag.vectorstore import VectorStore from rag.retriever import Retriever from rag.llm import LLMController from rag.orchestrator import RagOrchestrator app = FastAPI() # Lazy", "initialization to avoid heavy startup if not used _embedding = None _store = None _retriever = None _llm = None _orchestrator = None def ensure_components(): global _embedding, _store, _retriever, _llm, _orchestrator if _embedding is not None: return index_path = os.environ.get(\"RAG_INDEX_PATH\", \"./rag_index\") os.makedirs(index_path, exist_ok=True) _embedding = EmbeddingService() _store = VectorStore(index_path=index_path) _retriever = Retriever(_embedding, _store,", "top_k=int(os.environ.get(\"RAG_TOP_K\", \"5\"))) _llm = LLMController() _orchestrator = RagOrchestrator(_embedding, _store, _retriever, _llm, top_k=int(os.environ.get(\"RAG_TOP_K\", \"5\"))) class QueryRequest(BaseModel): query: str top_k: Optional[int] = None context: Optional[str] = None class QueryResponse(BaseModel): answer: str sources: List[str] score: float context: str class ChatMessage(BaseModel): role: str content: str class ChatCompletionRequest(BaseModel): model: str messages:", "List[ChatMessage] max_tokens: Optional[int] = 256 temperature: Optional[float] = 0.7 stream: Optional[bool] = False class ChatCompletionChoice(BaseModel): index: int message: ChatMessage finish_reason: str class ChatCompletionResponse(BaseModel): id: str object: str = \"chat.completion\" created: int model: str choices: List[ChatCompletionChoice] class CompletionChoice(BaseModel): text: str index: int logprobs: Optional[dict] = None finish_reason: str class CompletionResponse(BaseModel): id: str", "object: str = \"text_completion\" created: int model: str choices: List[CompletionChoice] class CompletionRequest(BaseModel): model: str prompt: str max_tokens: Optional[int] = 256 temperature: Optional[float] = 0.7 class ModelInfo(BaseModel): id: str object: str = \"model\" created: int owned_by: str = \"rag\" class ModelsResponse(BaseModel): object: str = \"list\" data: List[ModelInfo] @app.get(\"/health\") def health(): return {\"status\": \"ok\"} @app.get(\"/v1/models\") def list_models():", "logging.info(\"Models endpoint called\") import time return ModelsResponse( data=[ ModelInfo( id=\"mistralai/devstral-2512:free\", created=int(time.time()), owned_by=\"rag\" ) ] ) @app.post(\"/v1/chat/completions\") def chat_completions(request: Request, req: ChatCompletionRequest): try: logging.info(\"Chat completions called with model: \" + req.model + \", stream: \" + str(getattr(req, 'stream', False))) ensure_components() logging.info(\"Components ensured\") global _orchestrator if _orchestrator is None:", "return {\"error\": \"server not initialized\"} # Extract the last user message as the query user_messages = [msg for msg in req.messages if msg.role == \"user\"] if not user_messages: return {\"error\": \"no user message\"} query = user_messages[-1].content auth_header = request.headers.get(\"authorization\") api_key = None if auth_header and auth_header.startswith(\"Bearer \"): api_key = auth_header[7:] logging.info(\"Query: \" + query) result = _orchestrator.answer(query, model=req.model,", "temperature=req.temperature or 0.7, api_key=api_key) logging.info(\"Result: \" + str(result)) answer = result.get(\"answer\", \"\") if isinstance(result, dict) else str(result) import time if req.stream: import json def generate(): yield \"data: \" + json.dumps({ \"id\": f\"rag-{hash(query)}\", \"object\": \"chat.completion.chunk\", \"created\": int(time.time()), \"model\": req.model, \"choices\": [{\"index\": 0, \"delta\": {\"role\": \"assistant\", \"content\": \"\"}, \"finish_reason\": None}] }) + \"\\n\\n\" yield \"data: \" +", "json.dumps({ \"id\": f\"rag-{hash(query)}\", \"object\": \"chat.completion.chunk\", \"created\": int(time.time()), \"model\": req.model, \"choices\": [{\"index\": 0, \"delta\": {\"content\": answer}, \"finish_reason\": None}] }) + \"\\n\\n\" yield \"data: \" + json.dumps({ \"id\": f\"rag-{hash(query)}\", \"object\": \"chat.completion.chunk\", \"created\": int(time.time()), \"model\": req.model, \"choices\": [{\"index\": 0, \"delta\": {}, \"finish_reason\": \"stop\"}] }) + \"\\n\\n\" yield \"data: [DONE]\\n\\n\" return StreamingResponse(generate(),", "media_type=\"text/plain\") else: return { \"id\": \"rag-\" + str(hash(query)), \"object\": \"chat.completion\", \"created\": int(time.time()), \"model\": req.model, \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": answer }, \"finish_reason\": \"stop\" } ] } except Exception as e: logging.error(\"Error in chat_completions: \" + str(e)) return {\"error\": str(e)} @app.post(\"/v1/completions\") def completions(request: Request, req: CompletionRequest): ensure_components() global _orchestrator if", "_orchestrator is None: return {\"error\": \"server not initialized\"} query = req.prompt auth_header = request.headers.get(\"authorization\") api_key = None if auth_header and auth_header.startswith(\"Bearer \"): api_key = auth_header[7:] result = _orchestrator.answer(query, model=req.model, temperature=req.temperature or 0.7, api_key=api_key) answer = result.get(\"answer\", \"\") if isinstance(result, dict) else str(result) import time return { \"id\": \"rag-\" + str(hash(query)), \"object\": \"text_completion\",", "\"created\": int(time.time()), \"model\": req.model, \"choices\": [ { \"text\": answer, \"index\": 0, \"finish_reason\": \"stop\" } ] } # Run instructions (for developers): uvicorn rag_server.main:app --reload --port 8000", "import os import json import requests class RagLocalProvider: def __init__(self, base_url=None, top_k=5, api_key=None, timeout=30): self.base_url = base_url or os.environ.get(\"RAG_LOCAL_BASE_URL\", \"http://localhost:8000\") self.top_k = top_k or int(os.environ.get(\"RAG_TOP_K\", 5)) self.api_key = api_key or os.environ.get(\"RAG_API_KEY\") or os.environ.get(\"OPENROUTER_API_KEY\") self.timeout = timeout def query(self, prompt): url = f\"{self.base_url.rstrip('/')}/query\" headers = {\"Content-Type\":", "\"application/json\"} if self.api_key: headers[\"Authorization\"] = f\"Bearer {self.api_key}\" payload = {\"query\": prompt, \"top_k\": self.top_k} resp = requests.post(url, json=payload, headers=headers, timeout=self.timeout) resp.raise_for_status() return resp.json()", "try: from .provider import RagLocalProvider except Exception: class RagLocalProvider: # fallback stub def __init__(self, *args, **kwargs): raise RuntimeError(\"RagLocalProvider is not available in this environment yet.\") def query(self, *args, **kwargs): raise RuntimeError(\"RagLocalProvider is not available in this environment yet.\") __all__ = [\"RagLocalProvider\"]", "import argparse import os import json from rag.embedding import EmbeddingService from rag.vectorstore import VectorStore def chunk_text(text, chunk_size=500): \"\"\"Split text into chunks of approximately chunk_size characters.\"\"\" words = text.split() chunks = [] current = \"\" for word in words: if len(current) + len(word) + 1 > chunk_size: if current: chunks.append(current.strip()) current = word else: chunks.append(word) else: current += \" \" + word if current: chunks.append(current.strip())", "return chunks def parse_tags(tags_str): \"\"\"Parse tags string like 'project:pi-rag,service:docs' into dict.\"\"\" tags = {} if tags_str: for pair in tags_str.split(','): if ':' in pair: key, value = pair.split(':', 1) tags[key.strip()] = value.strip() return tags def ingest_file(file_path, tags, index_path='./rag_index'): \"\"\"Ingest a single file into the vector store.\"\"\" if not os.path.exists(file_path): print(f\"File {file_path} not found.\") return with open(file_path, 'r', encoding='utf-8') as f:", "text = f.read() chunks = chunk_text(text) if not chunks: print(\"No text to ingest.\") return embedding_service = EmbeddingService() vectors = embedding_service.embed(chunks) metadata = [tags.copy() for _ in chunks] store = VectorStore(index_path) store.add(chunks, vectors, metadata) store.save() print(f\"Ingested {len(chunks)} chunks from {file_path} with tags {tags}.\") def main(): parser = argparse.ArgumentParser(description=\"Ingest codebase data into RAG memory.\") parser.add_argument('--file',", "required=True, help=\"Path to file to ingest.\") parser.add_argument('--tags', help=\"Tags in format key:value,key2:value2\") parser.add_argument('--index-path', default='./rag_index', help=\"Path to vector store index.\") args = parser.parse_args() tags = parse_tags(args.tags) ingest_file(args.file, tags, args.index_path) if __name__ == \"__main__\": main()", "from typing import List import hashlib try: from sentence_transformers import SentenceTransformer except Exception: SentenceTransformer = None class EmbeddingService: def __init__(self, model_name='all-MiniLM-L6-v2', use_cache=True): self.model_name = model_name self.use_cache = use_cache self._cache = {} self._model = None if SentenceTransformer is not None: try: self._model = SentenceTransformer(model_name) except Exception: self._model = None def embed(self, texts): vectors = [] for t in", "texts: if self.use_cache and t in self._cache: vectors.append(self._cache[t]) continue vec = None if self._model is not None: try: vec = self._model.encode(t, convert_to_numpy=True).tolist() except Exception: vec = None if vec is None: vec = self._fallback_embed(t) if self.use_cache: self._cache[t] = vec vectors.append(vec) return vectors def _fallback_embed(self, text, dim=128): # Deterministic, lightweight fallback embedding based on hash h = hashlib.sha256(text.encode('utf-8')).hexdigest()", "nums = [int(h[i:i+8], 16) for i in range(0, len(h), 8)] vec = [] for i in range(dim): val = nums[i % len(nums)] vec.append((val / float(2**32)) * 2.0 - 1.0) norm = sum(x * x for x in vec) ** 0.5 if norm == 0: return [0.0 for _ in vec] return [x / norm for x in vec]", "from __future__ import division class Retriever: def __init__(self, embedding_service, vector_store, top_k=5): self.embedding_service = embedding_service self.vector_store = vector_store self.top_k = top_k def retrieve(self, texts, metadata_filters=None): if not texts: return [] embeddings = self.embedding_service.embed(texts) query_vec = embeddings[0] if len(embeddings) > 1: dim = len(embeddings[0]) query_vec = [sum(vec[i] for vec in embeddings) / len(embeddings) for i in range(dim)] return", "self.vector_store.search(query_vec, top_k=self.top_k, metadata_filters=metadata_filters)", "import json import os from typing import List, Dict, Optional, Tuple import numpy as np class VectorStore: def __init__(self, index_path=None): self.index_path = index_path self.docs = [] self.vectors = [] self.metadata = [] if index_path and os.path.exists(index_path): self.load(index_path) def add(self, docs, vectors, metadata=None): if len(docs) != len(vectors): raise ValueError(\"docs and vectors length mismatch\") self.docs.extend(docs) self.vectors.extend(vectors) if metadata is not None:", "if len(metadata) != len(docs): raise ValueError(\"metadata length must match docs length\") self.metadata.extend(metadata) else: self.metadata.extend([{} for _ in docs]) def _cosine(self, a, b): if not a or not b: return 0.0 dot = sum(x * y for x, y in zip(a, b)) na = sum(x * x for x in a) ** 0.5 nb = sum(x * x for x in b) ** 0.5 if na == 0 or nb == 0: return 0.0 return dot / (na * nb) def search(self, query_vector, top_k=5, metadata_filters=None): if not self.vectors: return [] scores = [] for i,", "vec in enumerate(self.vectors): if metadata_filters: if not self._matches_filters(self.metadata[i], metadata_filters): continue s = self._cosine(query_vector, vec) scores.append((i, s)) scores.sort(key=lambda x: x[1], reverse=True) results = [] for idx, score in scores[:top_k]: results.append((self.docs[idx], score, self.metadata[idx])) return results def _matches_filters(self, item_meta, filters): for key, value in filters.items(): if key not in item_meta or item_meta[key] != value: return", "False return True def save(self, path=None): target = path or self.index_path if not target: raise ValueError(\"target path required to save index\") os.makedirs(target, exist_ok=True) np.save(os.path.join(target, 'vectors.npy'), np.array(self.vectors, dtype=float)) with open(os.path.join(target, 'docs.json'), 'w', encoding='utf-8') as f: json.dump(self.docs, f, ensure_ascii=False) with open(os.path.join(target, 'meta.json'), 'w', encoding='utf-8') as f: json.dump(self.metadata, f,", "ensure_ascii=False) def remove_by_metadata(self, filters): \"\"\"Remove entries matching all filters.\"\"\" indices_to_remove = [] for i, meta in enumerate(self.metadata): if self._matches_filters(meta, filters): indices_to_remove.append(i) # Remove in reverse order to maintain indices for i in reversed(indices_to_remove): del self.docs[i] del self.vectors[i] del self.metadata[i] def load(self, path=None): target = path or self.index_path if not target or not os.path.exists(target): return docs_file =", "os.path.join(target, 'docs.json') vectors_file = os.path.join(target, 'vectors.npy') meta_file = os.path.join(target, 'meta.json') if not os.path.exists(docs_file) or not os.path.exists(vectors_file) or not os.path.exists(meta_file): return with open(docs_file, 'r', encoding='utf-8') as f: self.docs = json.load(f) vectors = np.load(vectors_file) self.vectors = vectors.tolist() with open(meta_file, 'r', encoding='utf-8') as f: self.metadata = json.load(f)", "# Rag core package exports (robust, safe imports) try: from .embedding import EmbeddingService except Exception: class EmbeddingService: def __init__(self, *args, **kwargs): pass def embed(self, texts): return [[0.0] * 128 for _ in texts] try: from .vectorstore import VectorStore except Exception: class VectorStore: def __init__(self, *args, **kwargs): pass def search(self, query_vector, top_k=5): return [] try: from .retriever import Retriever except Exception: class Retriever: def", "__init__(self, *args, **kwargs): pass def retrieve(self, texts): return [] try: from .llm import LLMController except Exception: class LLMController: def __init__(self, *args, **kwargs): pass def generate(self, prompt, max_tokens=256): return \"Dummy answer\" try: from .orchestrator import RagOrchestrator except Exception: class RagOrchestrator: def __init__(self, *args, **kwargs): pass def answer(self, query): return {\"answer\": \"Dummy answer\", \"sources\": [], \"score\": 0.0, \"context\": \"\"} __all__ =", "[ \"EmbeddingService\", \"VectorStore\", \"Retriever\", \"LLMController\", \"RagOrchestrator\", ]", "import os import json import time import requests class LLMController: def __init__(self, api_key=None, base_url=None, model=None, timeout=60.0): self.api_key = api_key or os.environ.get('OPENROUTER_API_KEY') self.base_url = base_url or 'https://openrouter.ai' self.model = model or 'openrouter/free-model-base' self.timeout = timeout def generate(self, prompt, max_tokens=256, model=None, temperature=0.7, api_key=None): model = model or self.model api_key = api_key or self.api_key try: url =", "f\"{self.base_url}/api/v1/chat/completions\" headers = { \"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\" } data = { \"model\": model, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"max_tokens\": max_tokens, \"temperature\": temperature } response = requests.post(url, headers=headers, json=data, timeout=self.timeout) response.raise_for_status() result = response.json() return result[\"choices\"][0][\"message\"][\"content\"] except Exception as e: return f\"LLM generation failed:", "{str(e)}\"", "import sys from .embedding import EmbeddingService from .vectorstore import VectorStore from .retriever import Retriever from .llm import LLMController from .orchestrator import RagOrchestrator def main(): # Simple CLI example: python -m rag.cli <query> if len(sys.argv) < 2: print(\"Usage: python -m rag.cli '<query>'\") sys.exit(1) query = sys.argv[1] # Lightweight scaffolding using in-memory store emb = EmbeddingService() store = VectorStore(index_path=None) # Example docs docs = [\"OpenRouter is", "a free, open API for LLMs.\", \"RAG combines embeddings and retrieval to provide context-aware answers.\", \"All-MiniLM-L6-v2 is a compact embedding model suitable for fast inference.\"] vecs = emb.embed(docs) store.add(docs, vecs, metadata=[{\"source\": \"docs\"}] * len(docs)) store.save(\"/tmp/rag_index_example\") retr = Retriever(emb, store, top_k=2) llm = LLMController() ort = RagOrchestrator(emb, store, retr, llm, top_k=2) answer = ort.answer(query) print(\"Answer:\\n\" + str(answer)) if __name__ ==", "'__main__': main()", "from typing import List class RagOrchestrator: def __init__(self, embedding_service, vector_store, retriever, llm_controller, top_k: int = 5): self.embedding_service = embedding_service self.vector_store = vector_store self.retriever = retriever self.llm_controller = llm_controller self.top_k = top_k def answer(self, query: str, model=None, temperature=0.7, api_key=None) -> dict: # Step 1: Check logs (retrieve log-related entries) log_results = self.retriever.retrieve([query],", "metadata_filters={\"type\": \"log\"}) if self.retriever else [] log_context = \"\\n\".join([d for d, _s, _md in log_results]) if log_results else \"No relevant logs found.\" # Step 2: Match features (extract entities, retrieve) entities = self._extract_entities(query) feature_filters = {k: v for k, v in entities.items() if k in [\"project\", \"service\", \"feature\", \"device\"]} feature_results = self.retriever.retrieve([query], metadata_filters=feature_filters) if self.retriever else [] feature_context =", "\"\\n\".join([d for d, _s, _md in feature_results]) if feature_results else \"No matching features found.\" # Combine contexts full_context = f\"Logs:\\n{log_context}\\n\\nFeatures:\\n{feature_context}\" # Step 3: Orchestrate LLM prompt prompt = f\"Based on the following context:\\n{full_context}\\n\\nQuestion: {query}\\nAnswer:\" answer = self.llm_controller.generate(prompt, model=model, temperature=temperature, api_key=api_key) # Collect sources and scores all_results = log_results + feature_results docs = [d", "for d, _s, _md in all_results] score = max((s for _d, s, _md in all_results), default=0.0) if all_results else 0.0 return { \"answer\": answer, \"sources\": docs, \"score\": score, \"context\": full_context, } def _extract_entities(self, query: str) -> dict: \"\"\"Simple entity extraction: map keywords to tags.\"\"\" entities = {} lower_query = query.lower() # Define mappings (expand as needed) mappings = { \"project\": [\"pi-rag\", \"project\"], \"service\": [\"rag\", \"server\", \"llm\", \"embedding\"], \"feature\": [\"auth\",", "\"llm\", \"vector\", \"api\"], \"device\": [\"macos\", \"ios\", \"backend\", \"linux\"] } for tag, keywords in mappings.items(): for word in keywords: if word in lower_query: entities[tag] = word break return entities", "import argparse import json import os import hashlib from pathlib import Path from rag.embedding import EmbeddingService from rag.vectorstore import VectorStore def compute_file_hash(file_path): \"\"\"Compute SHA256 hash of file content.\"\"\" hash_sha256 = hashlib.sha256() with open(file_path, \"rb\") as f: for chunk in iter(lambda: f.read(4096), b\"\"): hash_sha256.update(chunk) return hash_sha256.hexdigest() def should_skip_file(file_path, exclusions): \"\"\"Check if file should be skipped based on", "exclusions.\"\"\" path = Path(file_path) for exclusion in exclusions: if exclusion in str(path) or path.match(exclusion): return True return False def infer_tags(file_path, tag_mappings): \"\"\"Infer tags from file path using mappings.\"\"\" tags = {} path_str = str(file_path).lower() for key, value in tag_mappings.items(): if key.lower() in path_str: tags.update(value) return tags def chunk_text(text, chunk_size=500): \"\"\"Split text into chunks.\"\"\" words = text.split() chunks = [] current = \"\" for word", "in words: if len(current) + len(word) + 1 > chunk_size: if current: chunks.append(current.strip()) current = word else: current += \" \" + word if current: chunks.append(current.strip()) return chunks def load_config(config_path): \"\"\"Load config from JSON file.\"\"\" if not os.path.exists(config_path): raise FileNotFoundError(f\"Config file {config_path} not found.\") with open(config_path, 'r') as f: return json.load(f) def process_file(file_path, config, store, update_mode=False): \"\"\"Process a single", "file: check hash, chunk, embed, store.\"\"\" file_hash = compute_file_hash(file_path) file_meta_key = {\"file_path\": str(file_path)} # Check if file changed (for update mode) if update_mode: for meta in store.metadata: if meta.get(\"file_path\") == str(file_path): if meta.get(\"file_hash\") == file_hash: print(f\"Skipping unchanged file: {file_path}\") return break # Remove old entries for this file store.remove_by_metadata(file_meta_key) # Read and process with open(file_path, 'r', encoding='utf-8') as", "f: text = f.read() chunks = chunk_text(text, config.get(\"chunk_size\", 500)) if not chunks: return embedding_service = EmbeddingService() try: vectors = embedding_service.embed(chunks) except Exception as e: print(f\"Embedding failed for {file_path}: {e}\") return # Tags: project + inferred + file metadata tags = {\"project\": config[\"project\"], \"file_path\": str(file_path), \"file_hash\": file_hash} tags.update(infer_tags(file_path, config.get(\"tag_mappings\", {}))) metadata = [tags.copy() for _ in", "chunks] store.add(chunks, vectors, metadata) print(f\"Processed {len(chunks)} chunks from {file_path}\") def bulk_ingest(folder, config_path, update=False): \"\"\"Bulk ingest from folder.\"\"\" config = load_config(config_path) index_path = config.get(\"index_path\", \"./rag_index\") exclusions = config.get(\"exclusions\", [\"__pycache__\", \".git\", \"build\", \"*.pyc\", \"*.log\"]) file_types = set(config.get(\"file_types\", [\".py\", \".md\"])) store = VectorStore(index_path) store.load() folder_path = Path(folder)", "processed = 0 for file_path in folder_path.rglob(\"*\"): if file_path.is_file() and file_path.suffix in file_types and not should_skip_file(file_path, exclusions): try: process_file(file_path, config, store, update) processed += 1 except Exception as e: print(f\"Error processing {file_path}: {e}\") store.save() print(f\"Bulk ingestion complete: {processed} files processed.\") def main(): parser = argparse.ArgumentParser(description=\"Bulk ingest codebase into RAG memory.\")", "parser.add_argument('--folder', required=True, help=\"Root folder to scan.\") parser.add_argument('--config', required=True, help=\"Path to config JSON.\") parser.add_argument('--update', action='store_true', help=\"Incremental update mode.\") args = parser.parse_args() bulk_ingest(args.folder, args.config, args.update) if __name__ == \"__main__\": main()"]